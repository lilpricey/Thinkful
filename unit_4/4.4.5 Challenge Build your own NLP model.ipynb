{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Build your own NLP model\n",
    "### Sentiment analysis of Movie Reviews on IMDB \n",
    "___\n",
    "The [dataset](https://www.kaggle.com/c/word2vec-nlp-tutorial/data) contains 25,000 movie reviews from IMDB.com. The reviews have been labeled with a sentiment score of 0 if the IMDB score is < 5, and a score of 1 if the IMDB score is > 7. No individual movie had more than 30 reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Loren\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read and examine the data\n",
    "df = pd.read_csv('labeledTrainData.tsv', sep='\\t')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Examine target variable\n",
    "plt.figure(figsize=(8,4))\n",
    "sns.countplot(x=\"sentiment\", data=df);\n",
    "plt.title('Sentiment')\n",
    "plt.xticks(np.arange(2),('Negative', 'Positive'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create cleaning function\n",
    "def text_cleaner(text):\n",
    "    #make lowercase\n",
    "    text = text.lower()\n",
    "    #remove line breaks\n",
    "    text = re.sub(r'<br \\/>','',text)\n",
    "    #remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>the classic war of the worlds by timothy hines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>the film starts with a manager nicholas bell g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  with all this stuff going down at the moment w...\n",
       "1  2381_9          1  the classic war of the worlds by timothy hines...\n",
       "2  7759_3          0  the film starts with a manager nicholas bell g...\n",
       "3  3630_4          0  it must be assumed that those who praised this...\n",
       "4  9495_8          1  superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When I first tried this, I tokenized the words and removed stop words first, but the CountVectorizer function didn't work, what happend here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the reviews\n",
    "#tokens = df['review'].apply(nltk.tokenize.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sw = stopwords.words('English')\n",
    "#print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make function to remove stop words\n",
    "#def remove_stopwords(text):\n",
    "#    text = [word for word in text if word not in sw]\n",
    "#    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stop words\n",
    "#tokens_filtered = tokens.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make function to lemmatize\n",
    "#def make_lemma(text):\n",
    " #   lemmatizer = WordNetLemmatizer()\n",
    " #   text = [lemmatizer.lemmatize(word) for word in text]\n",
    " #   return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize words\n",
    "#tokens_filtered = tokens_filtered.apply(make_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create countvectorizer tool and fit_transform the reviews\n",
    "cv = CountVectorizer(ngram_range=(1, 2), max_features=5000, analyzer='word', lowercase=True)\n",
    "test_bag = cv.fit_transform(df['review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to array\n",
    "bow_features = test_bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes w/ BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = df['sentiment']\n",
    "X = bow_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "naive_bayes.fit(X_train, Y_train)\n",
    "\n",
    "naive_bayes.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.842 , 0.8316, 0.848 , 0.836 , 0.8428, 0.8432, 0.8448, 0.8608,\n",
       "       0.838 , 0.8508])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(naive_bayes, X, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tf-idf tool\n",
    "tf_idf_v = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, analyzer='word', lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit & transform into features\n",
    "tf_idf_features = tf_idf_v.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform into array\n",
    "tf_idf_features = tf_idf_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract vocab\n",
    "tfidf_vocab = tf_idf_v.vocabulary_.items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "naive_bayes = BernoulliNB()\n",
    "\n",
    "naive_bayes.fit(X_train, Y_train)\n",
    "\n",
    "naive_bayes.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.842 , 0.8316, 0.848 , 0.836 , 0.8428, 0.8432, 0.8448, 0.8608,\n",
       "       0.838 , 0.8508])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(naive_bayes, X_tf_idf, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### They are exactly the same. This seems strange. I'll try a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514666666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Y = df['sentiment']\n",
    "X = bow_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8801333333333333"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.886 , 0.8796, 0.8956, 0.872 , 0.8724, 0.8788, 0.8796, 0.888 ,\n",
       "       0.8748, 0.8844])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X_tf_idf, Y, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So tf_idf in the logistic regression model is performing the best. I'll focus on that and try to increase it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8828"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tf-idf tool, including tri-grams\n",
    "tf_idf_v = TfidfVectorizer(ngram_range=(1, 3), max_features=5000, analyzer='word', lowercase=True)\n",
    "tf_idf_features = tf_idf_v.fit_transform(df['review'])\n",
    "tf_idf_features = tf_idf_features.toarray()\n",
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8868"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tf-idf tool, including tri-grams, increasing max_features\n",
    "tf_idf_v = TfidfVectorizer(ngram_range=(1, 3), max_features=10000, analyzer='word', lowercase=True)\n",
    "tf_idf_features = tf_idf_v.fit_transform(df['review'])\n",
    "tf_idf_features = tf_idf_features.toarray()\n",
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.888"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = stopwords.words('English')\n",
    "\n",
    "#create tf-idf tool, including tri-grams, increasing max_features, adding stopwords\n",
    "tf_idf_v = TfidfVectorizer(ngram_range=(1, 3), max_features=10000, analyzer='word', lowercase=True, stop_words=sw)\n",
    "tf_idf_features = tf_idf_v.fit_transform(df['review'])\n",
    "tf_idf_features = tf_idf_features.toarray()\n",
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8928"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create tf-idf tool, including tri-grams, increasing max_features, adding stopwords\n",
    "tf_idf_v = TfidfVectorizer(ngram_range=(1, 3), analyzer='word',max_features=15000, lowercase=True, stop_words=sw)\n",
    "tf_idf_features = tf_idf_v.fit_transform(df['review'])\n",
    "tf_idf_features = tf_idf_features.toarray()\n",
    "Y = df['sentiment']\n",
    "X_tf_idf = tf_idf_features\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tf_idf, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8924, 0.8884, 0.9036, 0.89  , 0.8816, 0.8812, 0.8976, 0.8936,\n",
       "       0.8844, 0.8972])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X_tf_idf, Y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
