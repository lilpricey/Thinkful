{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.6.6 Keras Challenge - Detecting Planes in Satelite Images\n",
    "Now take your Keras skills and go build another neural network. Pick your data set, but it should be one of abstract types, possibly even nonnumeric, and use Keras to make five implementations of your network. Compare them both in computational complexity as well as in accuracy and given that tradeoff decide which one you like best.\n",
    "\n",
    "Your dataset should be sufficiently large for a neural network to perform well (samples should really be in the thousands here) and try to pick something that takes advantage of neural networks’ ability to have both feature extraction and supervised capabilities, so don’t pick something with an easy to consume list of features already generated for you (though neural networks can still be useful in those contexts).\n",
    "\n",
    "Note that if you want to use an unprocessed image dataset, scikit-image is a useful package for converting to importable numerics.\n",
    "\n",
    "The [dataset](https://www.kaggle.com/rhammell/planesnet/data) contains satelite imagery, and the goal is to detect the presence of a plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Loren\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "from scipy import misc\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths for each image\n",
    "img_path = r\"..\\unit_6\\planesnet\"\n",
    "file_paths = glob.glob(path.join(img_path, '*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the images\n",
    "images = [imageio.imread(path) for path in file_paths]\n",
    "images = np.asarray(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 20, 20, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display image size\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize values to 1 from 0 to 255 (256 values of pixels)\n",
    "images = images / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve labels\n",
    "n_images = images.shape[0]\n",
    "labels = np.zeros(n_images)\n",
    "for i in range(n_images):\n",
    "    filename = path.basename(file_paths[i])[0]\n",
    "    labels[i] = int(filename[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25600, 20, 20, 3) train samples\n",
      "(6400, 20, 20, 3) test samples\n"
     ]
    }
   ],
   "source": [
    "# Print sample sizes\n",
    "print(x_train.shape, 'train samples')\n",
    "print(x_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display image\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "\n",
    "# Import the backend\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = np.asarray([images.shape[1], images.shape[2], images.shape[3]])\n",
    "image_size[0], image_size[1], image_size[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sizes: (25600, 20, 20, 3) (6400, 20, 20, 3) (25600, 2) (6400, 2)\n",
      "Flattened: (25600, 1200) (6400, 1200)\n"
     ]
    }
   ],
   "source": [
    "#Reshaping to a flattened version\n",
    "x_flat_train = x_train.reshape(x_train.shape[0], 20*20*3)\n",
    "x_flat_test = x_test.reshape(x_test.shape[0], 20*20*3)\n",
    "\n",
    "#Encoding the output\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "print('Original Sizes:', x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "print('Flattened:', x_flat_train.shape, x_flat_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Multi Layer Perceptron, simple sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 64)                76864     \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 81,154\n",
      "Trainable params: 81,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(64, activation='relu', input_shape=(x_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "#model.add(Flatten())\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              #optimizer=RMSprop(),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 2s 77us/step - loss: 0.5142 - acc: 0.7564 - val_loss: 0.4030 - val_acc: 0.8127\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 1s 54us/step - loss: 0.3973 - acc: 0.8148 - val_loss: 0.3424 - val_acc: 0.8348\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 1s 54us/step - loss: 0.3777 - acc: 0.8319 - val_loss: 0.3502 - val_acc: 0.8428\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 2s 60us/step - loss: 0.3444 - acc: 0.8442 - val_loss: 0.3223 - val_acc: 0.8216\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 2s 60us/step - loss: 0.3108 - acc: 0.8614 - val_loss: 0.2840 - val_acc: 0.8709\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 2s 62us/step - loss: 0.3357 - acc: 0.8538 - val_loss: 0.2966 - val_acc: 0.8841\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 1s 55us/step - loss: 0.3201 - acc: 0.8601 - val_loss: 0.3686 - val_acc: 0.8028\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 1s 55us/step - loss: 0.3204 - acc: 0.8632 - val_loss: 0.3210 - val_acc: 0.8384\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 1s 55us/step - loss: 0.3020 - acc: 0.8727 - val_loss: 0.3576 - val_acc: 0.8139\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 1s 54us/step - loss: 0.2969 - acc: 0.8686 - val_loss: 0.3064 - val_acc: 0.8531\n",
      "Test loss: 0.3063703704625368\n",
      "Test accuracy: 0.853125\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_flat_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_flat_test, y_test))\n",
    "score = model.evaluate(x_flat_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Multi Layer Perceptron, with added layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 128)               153728    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 166,274\n",
      "Trainable params: 166,274\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Start with a simple sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add dense layers to create a fully connected MLP\n",
    "# Note that we specify an input shape for the first layer, but only the first layer.\n",
    "# Relu is the activation function used\n",
    "model.add(Dense(128, activation='relu', input_shape=(x_flat_train.shape[1],)))\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "#model.add(Flatten())\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 2s 93us/step - loss: 0.2741 - acc: 0.8815 - val_loss: 0.2998 - val_acc: 0.8573\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 2s 94us/step - loss: 0.2678 - acc: 0.8853 - val_loss: 0.4181 - val_acc: 0.8075\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 2s 96us/step - loss: 0.2594 - acc: 0.8903 - val_loss: 0.2223 - val_acc: 0.9123\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 2s 90us/step - loss: 0.2638 - acc: 0.8853 - val_loss: 0.2400 - val_acc: 0.9038\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 2s 89us/step - loss: 0.2525 - acc: 0.8936 - val_loss: 0.3510 - val_acc: 0.8430\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 2s 90us/step - loss: 0.2626 - acc: 0.8853 - val_loss: 0.2281 - val_acc: 0.9094\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 2s 95us/step - loss: 0.2529 - acc: 0.8934 - val_loss: 0.3225 - val_acc: 0.8519\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 3s 99us/step - loss: 0.2563 - acc: 0.8893 - val_loss: 0.3473 - val_acc: 0.8272\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 2s 94us/step - loss: 0.2595 - acc: 0.8885 - val_loss: 0.2450 - val_acc: 0.8984\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 2s 94us/step - loss: 0.2497 - acc: 0.8933 - val_loss: 0.2197 - val_acc: 0.9148\n",
      "Test loss: 0.21974052369594574\n",
      "Test accuracy: 0.91484375\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_flat_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_flat_test, y_test))\n",
    "score = model.evaluate(x_flat_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .915 accuracy.\n",
    "\n",
    "### 3. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 41s 2ms/step - loss: 0.4615 - acc: 0.7902 - val_loss: 0.2596 - val_acc: 0.8877\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 41s 2ms/step - loss: 0.2267 - acc: 0.9085 - val_loss: 0.1425 - val_acc: 0.9453\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 40s 2ms/step - loss: 0.1640 - acc: 0.9361 - val_loss: 0.1110 - val_acc: 0.9555\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 41s 2ms/step - loss: 0.1295 - acc: 0.9500 - val_loss: 0.1034 - val_acc: 0.9598\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 40s 2ms/step - loss: 0.1089 - acc: 0.9584 - val_loss: 0.0858 - val_acc: 0.9659\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 40s 2ms/step - loss: 0.1034 - acc: 0.9627 - val_loss: 0.0783 - val_acc: 0.9703\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 42s 2ms/step - loss: 0.0909 - acc: 0.9668 - val_loss: 0.0795 - val_acc: 0.9702\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 40s 2ms/step - loss: 0.0811 - acc: 0.9712 - val_loss: 0.0888 - val_acc: 0.9650\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 41s 2ms/step - loss: 0.0751 - acc: 0.9725 - val_loss: 0.0638 - val_acc: 0.9764\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 41s 2ms/step - loss: 0.0680 - acc: 0.9752 - val_loss: 0.0632 - val_acc: 0.9753\n",
      "Test loss: 0.06323640322312712\n",
      "Test accuracy: 0.9753125\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(20, 20, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CNN with additional dense layer and smaller batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/10\n",
      "25600/25600 [==============================] - 46s 2ms/step - loss: 0.4368 - acc: 0.8036 - val_loss: 0.2095 - val_acc: 0.9136\n",
      "Epoch 2/10\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.1986 - acc: 0.9230 - val_loss: 0.1281 - val_acc: 0.9511\n",
      "Epoch 3/10\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.1467 - acc: 0.9433 - val_loss: 0.1133 - val_acc: 0.9581\n",
      "Epoch 4/10\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.1225 - acc: 0.9543 - val_loss: 0.0925 - val_acc: 0.9631\n",
      "Epoch 5/10\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.1058 - acc: 0.9617 - val_loss: 0.0711 - val_acc: 0.9759\n",
      "Epoch 6/10\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0922 - acc: 0.9657 - val_loss: 0.0782 - val_acc: 0.9714\n",
      "Epoch 7/10\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.0832 - acc: 0.9709 - val_loss: 0.0722 - val_acc: 0.9727\n",
      "Epoch 8/10\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0760 - acc: 0.9734 - val_loss: 0.0635 - val_acc: 0.9786\n",
      "Epoch 9/10\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0696 - acc: 0.9768 - val_loss: 0.0770 - val_acc: 0.9723\n",
      "Epoch 10/10\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0680 - acc: 0.9771 - val_loss: 0.0539 - val_acc: 0.9822\n",
      "Test loss: 0.05394881635322236\n",
      "Test accuracy: 0.9821875\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(20, 20, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .982 accuracy\n",
    "\n",
    "#### CNN with 25 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25600 samples, validate on 6400 samples\n",
      "Epoch 1/25\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.4055 - acc: 0.8188 - val_loss: 0.1977 - val_acc: 0.9169\n",
      "Epoch 2/25\n",
      "25600/25600 [==============================] - 42s 2ms/step - loss: 0.1814 - acc: 0.9310 - val_loss: 0.2421 - val_acc: 0.9025\n",
      "Epoch 3/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.1366 - acc: 0.9481 - val_loss: 0.1670 - val_acc: 0.9316\n",
      "Epoch 4/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.1094 - acc: 0.9597 - val_loss: 0.0990 - val_acc: 0.9648\n",
      "Epoch 5/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0936 - acc: 0.9665 - val_loss: 0.0726 - val_acc: 0.9730\n",
      "Epoch 6/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0854 - acc: 0.9699 - val_loss: 0.0595 - val_acc: 0.9794\n",
      "Epoch 7/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0724 - acc: 0.9744 - val_loss: 0.0620 - val_acc: 0.9778\n",
      "Epoch 8/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0679 - acc: 0.9763 - val_loss: 0.0522 - val_acc: 0.9814\n",
      "Epoch 9/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0600 - acc: 0.9791 - val_loss: 0.0535 - val_acc: 0.9812\n",
      "Epoch 10/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0591 - acc: 0.9802 - val_loss: 0.0425 - val_acc: 0.9850\n",
      "Epoch 11/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0532 - acc: 0.9824 - val_loss: 0.0445 - val_acc: 0.9861\n",
      "Epoch 12/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0502 - acc: 0.9831 - val_loss: 0.0426 - val_acc: 0.9872\n",
      "Epoch 13/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0488 - acc: 0.9840 - val_loss: 0.0380 - val_acc: 0.9881\n",
      "Epoch 14/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0455 - acc: 0.9842 - val_loss: 0.0521 - val_acc: 0.9842\n",
      "Epoch 15/25\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0434 - acc: 0.9855 - val_loss: 0.0631 - val_acc: 0.9775\n",
      "Epoch 16/25\n",
      "25600/25600 [==============================] - 43s 2ms/step - loss: 0.0395 - acc: 0.9868 - val_loss: 0.0439 - val_acc: 0.9866\n",
      "Epoch 17/25\n",
      "25600/25600 [==============================] - 47s 2ms/step - loss: 0.0379 - acc: 0.9860 - val_loss: 0.0407 - val_acc: 0.9881\n",
      "Epoch 18/25\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.0679 - val_acc: 0.9764\n",
      "Epoch 19/25\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.0349 - acc: 0.9892 - val_loss: 0.0442 - val_acc: 0.9872\n",
      "Epoch 20/25\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.0322 - acc: 0.9891 - val_loss: 0.0393 - val_acc: 0.9880\n",
      "Epoch 21/25\n",
      "25600/25600 [==============================] - 44s 2ms/step - loss: 0.0300 - acc: 0.9904 - val_loss: 0.0392 - val_acc: 0.9881\n",
      "Epoch 22/25\n",
      "25600/25600 [==============================] - 46s 2ms/step - loss: 0.0309 - acc: 0.9901 - val_loss: 0.0385 - val_acc: 0.9898\n",
      "Epoch 23/25\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0287 - acc: 0.9907 - val_loss: 0.0363 - val_acc: 0.9884\n",
      "Epoch 24/25\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0304 - acc: 0.9898 - val_loss: 0.0369 - val_acc: 0.9895\n",
      "Epoch 25/25\n",
      "25600/25600 [==============================] - 45s 2ms/step - loss: 0.0254 - acc: 0.9917 - val_loss: 0.0426 - val_acc: 0.9866\n",
      "Test loss: 0.04258300182011226\n",
      "Test accuracy: 0.9865625\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=(20, 20, 3)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=25,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .987 accuracy\n",
    "\n",
    "### Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
      "553467904/553467096 [==============================] - 42s 0us/step\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
      "96116736/96112376 [==============================] - 21s 0us/step\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
      "102858752/102853048 [==============================] - 29s 0us/step\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\n",
      "17227776/17225924 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    " \n",
    "#Load the VGG model\n",
    "vgg_model = vgg16.VGG16(weights='imagenet')\n",
    " \n",
    "#Load the Inception_V3 model\n",
    "inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "#Load the ResNet50 model\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "#Load the MobileNet model\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'..\\\\unit_6\\\\planesnet\\\\0__20140723_181317_0905__-122.073653222_37.7090043618.png'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PIL image size (224, 224)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFhxJREFUeJzt3X2spGV5x/Hvb+a87S7vr6VAy0uxqf7RlW6sidXYWhVI42oTFdIotcbVFBJNbFrQpDUmTfoimpq2NGsgQmNBG3zhD2wlxGj8AxUUEURkQZSVzaJodld295wzM1f/mOfo3Ms57nWfmTkzp/l9kpMzc597nrlmnjnXeWbO9Vy3IgIzsxWtSQdgZtPFScHMCk4KZlZwUjCzgpOCmRWcFMysMLakIOkySY9K2iPpunHdj5mNlsZRpyCpDXwPeDWwF/g6cFVEfGfkd2ZmIzWuI4WXAHsi4omIWAJuB3aO6b7MbIRmxrTdc4GnBq7vBX5/rcknnbgtzjrjlJEGIKlqflBzxJTbdquVj0HJba7HOI4Gq7aYvP+6fVAXRPb1UPO6qZlbvQ+ym67Y7mNP/OgnEXHm8eaNKyms9pCK6CXtAnYBnHn6ydzwwb9MbbgXvdS8mfZsal7tdgGU/GXfsrCQ3ubsTD7ebi8fK0Cns5yaV/PCrQkhG2+328lvFOhFNz13fjb3/M6059LbnJlpp+f2KvdZNil0O/nn7PI3//UPMvPG9fZhL3D+wPXzgKcHJ0TE7ojYERE7Tjpx25jCMLNa40oKXwcukXShpDngSuDOMd2XmY3QWN4+RERH0rXA/wJt4OaIeHgc92VmozWuzxSIiLuAu8a1fTMbD1c0mlnBScHMCk4KZlZwUjCzgpOCmRXG9t+HGpLS1WG9yOWxXi9f7dafn6/myxb+LVVUm80mK+6grnwa8uW43aqiu4rnK/nctpSvEAQ4uriYnttLPrhtW/MxSOPbZ93k67fdHv3fdR8pmFnBScHMCk4KZlZwUjCzgpOCmRWcFMys4KRgZgUnBTMrOCmYWWHdFY2SzgduBX4N6AG7I+JfJH0AeAfw42bq+5reCr9CNJs4vmxhWC+5vRWqSI+9brIRaUWVJBXVfK1WZRPQZI/GmirQmkaz2R6RNX0yId93EeDo0lJqXk0V6tx8/jmYma37VYtcuCwvV/Z+TBimzLkDvDciviHpROB+SXc3P/tIRHxo+PDMbKOtOylExD5gX3P5kKRH6Ld2N7NNbCSfKUi6AHgx8NVm6FpJD0q6WdKpo7gPM9sYQycFSScAdwDviYiDwI3AxcB2+kcSN6xxu12S7pN034FDzw0bhpmNyFBJQf1zR+8APhERnwaIiP0R0Y2IHvAx+kvIPc/gug8ne90Hs6mx7qSg/kn6NwGPRMSHB8bPGZj2BuCh9YdnZhttmP8+vAx4C/BtSQ80Y+8DrpK0nf7/GZ8E3jlUhGa2oYb578NXWH3FO6/1YLaJuaLRzApOCmZWcFIws8JUdHMOoNvL1XDPtnMhR+Rr2Pvz83NrO/Nm1HTlnZ2dq9r28nLu3INu72h6m71uTbfs3L6t22ZdN+UtW7am5rXaFeegjHGfZTtw95K/NzV8pGBmBScFMys4KZhZwUnBzApOCmZWcFIws4KTgpkVnBTMrOCkYGYFJwUzK0xFmTMBnXS5Zq4Utt2qe2jZMmuASLY371SU7daUq7aSJbArZmdy5cA1LemzbdsB2q3c356FhbpS4Lm5hfzc+eTcVr50en4u/xqbq2hHDxVLGbjM2czGbegjBUlPAofo/wnvRMQOSacBnwQuoN996U0R8bNh78vMxm9URwp/GBHbI2JHc/064J6IuAS4p7luZpvAuN4+7ARuaS7fArx+TPdjZiM2iqQQwBck3S9pVzN2drOC1MpKUmcdeyOv+2A2nUbx34eXRcTTks4C7pb03cyNImI3sBvgty48t3LFVDMbl6GPFCLi6eb7M8Bn6C/+sn9l/Yfm+zPD3o+ZbYxhV4ja1qw4jaRtwGvoL/5yJ3B1M+1q4HPD3I+ZbZxh3z6cDXym6Sc3A/xXRPyPpK8Dn5L0duCHwBuHvB8z2yBDJYWIeAL43VXGnwVeVbEdOp1cZVYkj22yVXTr0Upuu5VsMgvQ6dQ0La37CGZmJteMdKai6q5bUa05Pz+fmrd1a6656opsM9Z+DLmKxmzDVIDDR46k50ZNZ2DyzYHnZkdflOyKRjMrOCmYWcFJwcwKTgpmVnBSMLOCk4KZFZwUzKzgpGBmBScFMys4KZhZYSoat6rVYnY2Vwq7vLyUmhdRUzYMyV6sAOSKhqHdzs6Eml6sUl0u37KQK/GtaQJ6ZC63v2ru/4QTTkhvE2BhPt/otdPNlRkr8s/BbDu/Hw4dOpieC9BKb3v0XQd8pGBmBScFMys4KZhZwUnBzArr/qBR0m/TX9thxUXA3wKnAO8AftyMvy8i7lp3hGa2odadFCLiUWA7gKQ28CP6PRrfBnwkIj40kgjNbEON6u3Dq4DHI+IHI9qemU3IqJLClcBtA9evlfSgpJslnTqi+zCzDTB0UpA0B7wO+O9m6EbgYvpvLfYBN6xxu18sBnPwoBeDMZsWozhSuBz4RkTsB4iI/RHRjYge8DH660A8T0TsjogdEbHjpJO2jSAMMxuFUZQ5X8XAWwdJ56wsGQe8gf46EMeVLfOdTXav7XQ6uQ02arrt5jNpfps1Zc4zFeW1AAsLua7HNd2RO718GXm2LHuh4v4BZiqehu7ycmreT392IL3N5aVcyT1Ar1f3ejx6NDc/Xw6dN1RSkLQVeDXwzoHhf5K0nf5vxJPH/MzMptyw6z4cBk4/ZuwtQ0VkZhPlikYzKzgpmFnBScHMCk4KZlZwUjCzgpOCmRWcFMys4KRgZgUnBTMrTEWLd4h0S3Yle7HXtFcH6HTztenZ8yQqTqcgIn/yQ3um7rFt25ZrsT43vyW9zZqa+2zr+E4nfy4BwJHDR9NzO0u5cx+iosX7YvJ8CoBuN7/dmjg6y3XnVGT4SMHMCk4KZlZwUjCzgpOCmRWcFMyskEoKTQPWZyQ9NDB2mqS7JT3WfD+1GZekj0ra0zRvvXRcwZvZ6GWPFD4OXHbM2HXAPRFxCXBPcx36PRsvab520W/kamabRCopRMSXgZ8eM7wTuKW5fAvw+oHxW6PvXuAUSeeMIlgzG79hPlM4e6VBa/P9rGb8XOCpgXl7mzEz2wTGUdG4Wmne82r7JO2i//aCM08/GbKVZMm2x62a9shAK1kpCaRLFVURw0xNBabqdtvM3Fxq3tZt+YrGmXY+hm43V616+Ll8h2iAReVLRhcXc9WS2XkAR48eSc/tVnS/BiBZBVrRMDxtmCOF/StvC5rvzzTje4HzB+adBzx97I2LdR9O9LoPZtNimKRwJ3B1c/lq4HMD429t/gvxUuDAwDoQZjblUseAkm4DXgmcIWkv8HfAPwCfkvR24IfAG5vpdwFXAHuAw/RXoTazTSKVFCLiqjV+9KpV5gZwzTBBmdnkuKLRzApOCmZWcFIws4KTgpkVnBTMrOCkYGaFqWjcGkS6uWe2dDjb4PUXMVSUofaSpaVayMcwNzebnruwkCtbXtGaye3mdnIewOxsfm67m/vbc+Rw3d+ommaoy51ck9VOsiQboNPJz13uLKbnArSVey5qSumzfKRgZgUnBTMrOCmYWcFJwcwKTgpmVnBSMLOCk4KZFZwUzKzgpGBmheMmhTUWgvlnSd9tFnv5jKRTmvELJB2R9EDz9R/jDN7MRi9Tq/px4F+BWwfG7gauj4iOpH8Ergf+pvnZ4xGxvSqKgG6yzDmS86q6IwOR7NAMoGQJaquVL0Gdn59Pz12Yrytzzj8X+ecgux/6c3PlwAsLC+ltQl2pdaud22fZcmiAXtR0AK87KI/kvphImfNqC8FExBciotNcvZd+x2Yz+39gFJ8p/AXw+YHrF0r6pqQvSXr5WjeStEvSfZLuO/jzwyMIw8xGYaizJCW9H+gAn2iG9gG/ERHPSvo94LOSXhQRB4+9bUTsBnYDXHzBr49hSQszW491HylIuhr4E+DPmg7ORMRiRDzbXL4feBx4wSgCNbONsa6kIOky+h8svi4iDg+Mnymp3Vy+iP7K00+MIlAz2xjHffuwxkIw1wPzwN3Np5/3RsS7gFcAH5TUAbrAuyLi2NWqzWyKHTcprLEQzE1rzL0DuGPYoMxsclzRaGYFJwUzK0xF49a+XGVWN9k1tdPNV6YBbN2yNT13fn5Lat7JJ51csc18Nd9cRSUfgCJXfXj0yJH0NmMuH28vuS86y0vpbQLMzOYrO7ckqyVP2JZ/HUS2gy+wVFl4mK0YlUb/33wfKZhZwUnBzApOCmZWcFIws4KTgpkVnBTMrOCkYGYFJwUzKzgpmFnBScHMCk4KZlaYinMfIoKlZN37bDtX797p5joI/yIG5bs/n3TSKal5p556anqbW7fkzyU4slh3XkdWVS3/0cX8dpOb7XTy2+zP7xx/0koMyb9/CxXnoCh5vg7A0lLdPltOzu9F3es8Y73rPnxA0o8G1ne4YuBn10vaI+lRSa8decRmNlaZ9Plx4LJVxj8SEdubr7sAJL0QuBJ4UXObf19pz2Zmm8O61n34FXYCtzcNXL8P7AFeMkR8ZrbBhvmg8dpm2bibJa28eT4XeGpgzt5m7Hm87oPZdFpvUrgRuBjYTn+thxua8dU+eVn1Y6aI2B0ROyJix0kn5BtbmNl4rSspRMT+iOhGRA/4GL98i7AXOH9g6nnA08OFaGYbab3rPpwzcPUNwMp/Ju4ErpQ0L+lC+us+fG24EM1sI6133YdXStpO/63Bk8A7ASLiYUmfAr5Dfzm5ayLG8I9UMxubka770Mz/e+DvhwnKzCbHZc5mVpiKMmcErWSNUy/Z+nrrllwb9hVbKlq8b1nIbXt2Jt+CPNviHmB5qa4c+MCBA6l5C3M18eZLorPl04eP5lvM9zc8+nemy53842q18vtsfm62Ko52a3J/r32kYGYFJwUzKzgpmFnBScHMCk4KZlZwUjCzgpOCmRWcFMys4KRgZoWpqGgUYnYmV/El5arIZtt1FWQLc/PpuTMzyerLyFVfAhw8lG800+vlG5ZCvsnq4Z/nKwo73VyjXYBIVqFGxfMFsLiUj2GmnXvdtNo13QPzf1PnZup+1dK1ksnfhxo+UjCzgpOCmRWcFMyssN51Hz45sObDk5IeaMYvkHRk4Gf/Mc7gzWz0Mp9+fBz4V+DWlYGIePPKZUk3AIPn5j4eEdtHFaCZbaxM56UvS7pgtZ+p/6+ANwF/NNqwzGxShv1M4eXA/oh4bGDsQknflPQlSS8fcvtmtsGGrVO4Crht4Po+4Dci4llJvwd8VtKLIuLgsTeUtAvYBXDG6ScPGYaZjcq6jxQkzQB/CnxyZaxZLu7Z5vL9wOPAC1a7vReDMZtOw7x9+GPguxGxd2VA0pkrC8pKuoj+ug9PDBeimW2kda37EBE30V9d+rZjpr8C+KCkDtAF3hURx12cVhIzyfLSbJmzKppqAnQrynZ//tzPU/M6neX0NmtKoo8cqVt7s9fLNThdXsrHWzN3Kdlott2u+xuVfS0ALCaf37mKBqvZ0nyApWSp94rRFy/nrXfdByLiz1cZuwO4Y/iwzGxSXNFoZgUnBTMrOCmYWcFJwcwKTgpmVnBSMLOCk4KZFZwUzKzgpGBmhano5hxAN9vxN1myG5UxdCs6JC93ciXRBw7mi1Vrymuz3ZFXdLu552zxaL6b89JSXUfpjF6vrri3XdF5udXK/f3rdfOvnGhVvMpq5pJ//Wb3bQ0fKZhZwUnBzApOCmZWcFIws4KTgpkVMus+nC/pi5IekfSwpHc346dJulvSY833U5txSfqopD2SHpR06bgfhJmNTuZIoQO8NyJ+B3gpcI2kFwLXAfdExCXAPc11gMvpt2G7hH5j1htHHrWZjc1xk0JE7IuIbzSXDwGPAOcCO4Fbmmm3AK9vLu8Ebo2+e4FTJJ0z8sjNbCyqPlNoFoV5MfBV4OyI2Af9xAGc1Uw7F3hq4GZ7mzEz2wTSSUHSCfT7L75ntXUcBqeuMva8Ai1JuyTdJ+m+g4eey4ZhZmOWSgqSZuknhE9ExKeb4f0rbwua788043uB8wdufh7w9LHbLNZ9OHHbeuM3sxHLtHgXcBPwSER8eOBHdwJXA//QfP/cwPi1km4Hfh84sPI24zj3lAq4lywKX1zM1/HXbBdgdjF3gDU3N5feZmc532I+Ks/sWFrKbbvmzINu8hwUyJ930Grnz/+olW4HHxXnPkT+OejVvMDIt/yvaXOflTkh6mXAW4Bvryw5D7yPfjL4lKS3Az8E3tj87C7gCmAPcBh420gjNrOxyqz78BXW/iPyqlXmB3DNkHGZ2YS4otHMCk4KZlZwUjCzgpOCmRWcFMys4KRgZgUnBTMrTEU3ZwlmZ3L5SZELuVPX8Ji28jdY7uQq2dTKdzxuVXT7reliDNBLdn9eXs7HO19RrZmtaKxV8zzMJOeqItRuN/+aUeVvWi9ZMdqpiCHLRwpmVnBSMLOCk4KZFZwUzKzgpGBmBScFMys4KZhZwUnBzApOCmZWcFIws4KiolHl2IKQfgw8B/xk0rEM4Qw2d/yw+R/DZo8fxvsYfjMizjzepKlICgCS7ouIHZOOY702e/yw+R/DZo8fpuMx+O2DmRWcFMysME1JYfekAxjSZo8fNv9j2OzxwxQ8hqn5TMHMpsM0HSmY2RSYeFKQdJmkRyXtkXTdpOPJkvSkpG9LekDSfc3YaZLulvRY8/3UScc5SNLNkp6R9NDA2Koxq++jzX55UNKlk4v8F7GuFv8HJP2o2Q8PSLpi4GfXN/E/Kum1k4n6lySdL+mLkh6R9LCkdzfj07UPImJiX0AbeBy4CJgDvgW8cJIxVcT+JHDGMWP/BFzXXL4O+MdJx3lMfK8ALgUeOl7M9NcD/Tz9JQNfCnx1SuP/APBXq8x9YfN6mgcubF5n7QnHfw5waXP5ROB7TZxTtQ8mfaTwEmBPRDwREUvA7cDOCcc0jJ3ALc3lW4DXTzCW54mILwM/PWZ4rZh3ArdG373AKZLO2ZhIV7dG/GvZCdweEYsR8X36Cx6/ZGzBJUTEvoj4RnP5EPAIcC5Ttg8mnRTOBZ4auL63GdsMAviCpPsl7WrGzo6IfdB/AQBnTSy6vLVi3kz75trm8PrmgbdsUx2/pAuAFwNfZcr2waSTwmqrWW+Wf4e8LCIuBS4HrpH0ikkHNGKbZd/cCFwMbAf2ATc041Mbv6QTgDuA90TEwV81dZWxsT+GSSeFvcD5A9fPA56eUCxVIuLp5vszwGfoH5ruXzm8a74/M7kI09aKeVPsm4jYHxHdiOgBH+OXbxGmMn5Js/QTwici4tPN8FTtg0knha8Dl0i6UNIccCVw54RjOi5J2ySduHIZeA3wEP3Yr26mXQ18bjIRVlkr5juBtzafgL8UOLByiDtNjnmP/Qb6+wH68V8paV7ShcAlwNc2Or5BkgTcBDwSER8e+NF07YNJfho78Anr9+h/Ovz+SceTjPki+p9sfwt4eCVu4HTgHuCx5vtpk471mLhvo3+IvUz/r9Db14qZ/qHrvzX75dvAjimN/z+b+B6k/0t0zsD89zfxPwpcPgXx/wH9w/8HgQearyumbR+4otHMCpN++2BmU8ZJwcwKTgpmVnBSMLOCk4KZFZwUzKzgpGBmBScFMyv8H+eBnOWnOEYIAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array size (224, 224, 3)\n",
      "image batch size (1, 224, 224, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2128435c518>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "filename = file_paths[30000]\n",
    "# load an image in PIL format\n",
    "original = load_img(filename, target_size=(224, 224))\n",
    "print('PIL image size',original.size)\n",
    "plt.imshow(original)\n",
    "plt.show()\n",
    " \n",
    "# convert the PIL image to a numpy array\n",
    "# IN PIL - image is in (width, height, channel)\n",
    "# In Numpy - image is in (height, width, channel)\n",
    "numpy_image = img_to_array(original)\n",
    "plt.imshow(np.uint8(numpy_image))\n",
    "plt.show()\n",
    "print('numpy array size',numpy_image.shape)\n",
    " \n",
    "# Convert the image / images into batch format\n",
    "# expand_dims will add an extra dimension to the data at a particular axis\n",
    "# We want the input matrix to the network to be of the form (batchsize, height, width, channels)\n",
    "# Thus we add the extra dimension to the axis 0.\n",
    "image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "print('image batch size', image_batch.shape)\n",
    "plt.imshow(np.uint8(image_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
      "40960/35363 [==================================] - 0s 1us/step\n",
      "[[('n04548280', 'wall_clock', 0.06821078), ('n04209239', 'shower_curtain', 0.039430827), ('n03888257', 'parachute', 0.021995947), ('n02782093', 'balloon', 0.021237824), ('n03532672', 'hook', 0.013352058)]]\n"
     ]
    }
   ],
   "source": [
    "# prepare the image for the VGG model\n",
    "processed_image = vgg16.preprocess_input(image_batch.copy())\n",
    " \n",
    "# get the predicted probabilities for each class\n",
    "predictions = vgg_model.predict(processed_image)\n",
    "# print predictions\n",
    " \n",
    "# convert the probabilities to class labels\n",
    "# We will get top 5 predictions which is the default\n",
    "label = decode_predictions(predictions)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The pretrained model doesn't work on the dataset photos as they have been reduced down to grainy blobs, probably to make training a lot faster. Out of the models from scratch, the 4th model I believe is the best one, it had an accuracy of .982 and it trained a lot faster than model 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
